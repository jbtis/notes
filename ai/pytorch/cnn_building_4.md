
### Understanding the layer parameters and arguments 


```python
import torch
import torch.nn as nn
```


```python
class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__() #calling the super constructor (nn module)
        
        #parameters vs arguments: parameters are the local variables of the function while the arguments 
        #are the values passed to the function
        
        #convolutional layers
        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)
        self.conv2 = nn.Conv2d(in_channels = 6, out_channels =12, kernel_size = 5)
        
        #increase out_channels as we add convolutional layers
        
        #linear layers
        self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)
        self.fc2 = nn.Linear(in_features = 120, out_features = 60)
        
        #decrease out_features as we add fully connected layers
        
        
        #output layer
        self.out = nn.Linear(in_features = 60, out_features = 10)
        
        #10 because there are 10 items we want to categorize
    
    def forward(self, t):
        t = self.layer(t) #layer function is not defined but we can sill dclare it beacuse of dyanmic typing
        return t
        
        
```

#### What does each argument do?

*Note: The arguments passed to the layers in the constructor are hyperparameters. Hyperparameters refer to values that are chosen manually by the programmer based on trial and error. This values are by no means derived (or learned) by the neural network.*

Data Dependent Hyperparameters
* in_channels (depends on number of color channels of the image) (on hidden layers, depends on previous output)
* in_features (depends on how the data is being reoresented) (on hidden layers, depends on previous output)
* out_features (depends on how many items we want to  categorize)

Hyperparameters

* kernel_size: Sets the size of the kernel(or filter) to be convolved with the input channel
* out_channels: Sets the number of kernels, since each kernel produces and output channel (or a feature map)
* out_features: Size of output tensor

Learnable Parameters

The parameters that the neural network adjust every iteration to minimize loss function. These are the weights of each layer. At the start, they are chosen at random


```python
net = Network()
```


```python
print(net)
```

    Network(
      (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
      (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
      (fc1): Linear(in_features=192, out_features=120, bias=True)
      (fc2): Linear(in_features=120, out_features=60, bias=True)
      (out): Linear(in_features=60, out_features=10, bias=True)
    )
    


```python
net.conv1
```




    Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))




```python
net.conv2
```




    Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))




```python
net.fc1
```




    Linear(in_features=192, out_features=120, bias=True)




```python
net.fc2
```




    Linear(in_features=120, out_features=60, bias=True)




```python
net.out
```




    Linear(in_features=60, out_features=10, bias=True)




```python
net.conv1.weight #Parameter class extends tensor class
#Since we specfied 6 out_channe;s with 5x5 kernels we get 6 5x5 kernels
```




    Parameter containing:
    tensor([[[[-0.0716, -0.1685,  0.0279, -0.1634, -0.0750],
              [ 0.1527,  0.0693,  0.0200,  0.0867,  0.1350],
              [-0.1232, -0.0322,  0.1240,  0.1040,  0.1753],
              [-0.1618, -0.1457,  0.1518,  0.0437, -0.1656],
              [ 0.0665,  0.1841, -0.1820,  0.1507, -0.0651]]],
    
    
            [[[ 0.0073, -0.1399, -0.0204,  0.0134,  0.1799],
              [ 0.0996, -0.0679,  0.0796, -0.1572,  0.0899],
              [ 0.0408, -0.0847, -0.1107,  0.0805, -0.0203],
              [-0.0707, -0.0915,  0.0329,  0.1579,  0.1029],
              [ 0.1497,  0.1033, -0.1100,  0.0248,  0.0505]]],
    
    
            [[[-0.0267,  0.1487, -0.1817,  0.1249,  0.0985],
              [ 0.1815, -0.0550, -0.0527,  0.1983,  0.1631],
              [ 0.1381, -0.1160, -0.1618,  0.1481,  0.0759],
              [ 0.1996, -0.1446, -0.1745,  0.0290,  0.0079],
              [ 0.0332,  0.1810,  0.0502,  0.1535, -0.1295]]],
    
    
            [[[ 0.1348,  0.1557,  0.0336,  0.0914, -0.1393],
              [ 0.1627,  0.0004, -0.1732, -0.0633, -0.0403],
              [ 0.1663,  0.0649, -0.1116,  0.1650, -0.0682],
              [-0.1873,  0.1794, -0.0324,  0.0887, -0.1150],
              [-0.1930, -0.0085, -0.1644,  0.0343,  0.1269]]],
    
    
            [[[ 0.0963,  0.0442,  0.1512, -0.0565,  0.0090],
              [ 0.0972,  0.1924, -0.1257, -0.0238, -0.0595],
              [-0.0931,  0.0946,  0.1902, -0.1362, -0.1604],
              [-0.0997,  0.0435,  0.0936, -0.1567,  0.0213],
              [ 0.1130,  0.1999,  0.0213, -0.1636, -0.1835]]],
    
    
            [[[-0.1969,  0.1735, -0.0168,  0.1843, -0.0205],
              [-0.0935, -0.0157, -0.0122, -0.1564, -0.0260],
              [ 0.1074, -0.0184,  0.1296,  0.1586,  0.0323],
              [ 0.1513, -0.0528,  0.1170, -0.0217,  0.0948],
              [ 0.0436,  0.1076, -0.1196, -0.0403,  0.0649]]]], requires_grad=True)




```python
net.conv2.weight
```




    Parameter containing:
    tensor([[[[ 6.1402e-02, -1.6727e-02, -7.1164e-02,  2.4655e-02,  3.5027e-04],
              [ 9.1002e-03,  7.6012e-02, -5.3289e-02, -4.2654e-04,  5.3623e-02],
              [-7.4581e-02,  4.4020e-02,  2.4549e-02,  4.5146e-02, -2.6197e-02],
              [-4.5635e-02,  6.2771e-02,  5.6523e-02,  1.5837e-02,  5.0150e-02],
              [-7.9738e-02,  7.4341e-02, -4.9406e-02, -4.3201e-02,  3.2361e-02]],
    
             [[-1.8044e-02, -9.1371e-03, -1.2172e-02,  5.3365e-02, -6.7301e-02],
              [-6.2240e-02, -5.9164e-02, -9.9175e-05, -7.8145e-02, -3.6735e-02],
              [ 3.2967e-02,  3.4532e-02,  3.7146e-02, -5.3073e-02, -4.1571e-02],
              [-2.5905e-02, -4.8252e-02, -5.7689e-02, -7.0814e-02,  1.7978e-02],
              [-6.0483e-02,  6.5688e-02,  7.7841e-02,  6.3535e-02,  2.0769e-02]],
    
             [[ 7.2342e-02, -5.2255e-02,  6.9638e-02,  1.5469e-03, -6.2089e-02],
              [ 6.7596e-02, -5.6768e-02,  2.1165e-02, -2.0821e-02, -7.0014e-02],
              [ 2.9622e-02, -3.8434e-02,  4.1197e-02,  2.6415e-02,  6.1622e-02],
              [-4.8351e-02, -7.4104e-02,  1.0545e-02, -5.3924e-02,  5.0172e-02],
              [ 6.8333e-04,  7.1221e-02,  1.9744e-02, -2.7154e-02,  2.5544e-02]],
    
             [[-7.4597e-02, -7.4095e-02, -5.9831e-02, -2.0727e-02, -4.4161e-02],
              [-7.5848e-02,  7.4571e-02,  5.5074e-02,  3.4138e-02,  6.4267e-02],
              [ 4.8081e-02,  1.7402e-02,  7.2386e-02,  2.5795e-02,  7.0265e-02],
              [-2.5258e-02, -5.7379e-02,  9.8599e-03, -5.6944e-02,  1.5142e-02],
              [-7.4775e-02, -3.3302e-02, -5.3164e-02,  7.0091e-02,  4.9705e-02]],
    
             [[ 5.9637e-02,  2.1695e-02, -3.4162e-02, -7.8482e-02, -6.9402e-02],
              [-6.8736e-02,  4.4803e-02,  7.0033e-02, -3.7022e-02,  5.4126e-02],
              [ 5.5726e-02,  4.3352e-02, -1.0609e-02,  3.6062e-02, -2.0380e-02],
              [-4.8029e-02, -8.0567e-02, -2.7039e-02,  1.4184e-02, -6.9239e-02],
              [ 2.6395e-02, -1.5623e-02, -9.7665e-03,  2.4182e-02, -7.4942e-02]],
    
             [[-7.2303e-02, -8.0774e-02, -4.9205e-02,  7.3249e-02,  6.2880e-02],
              [ 3.8068e-02, -1.7241e-02,  7.7009e-02,  2.2750e-02,  4.3237e-02],
              [ 3.0589e-02, -6.4878e-02,  7.1319e-02, -5.8570e-02,  3.0786e-02],
              [-1.1632e-02,  4.6518e-02,  2.3691e-02,  6.7852e-02,  5.3309e-02],
              [-7.5337e-02, -7.7950e-02,  6.0156e-02, -4.4766e-02,  6.1774e-02]]],
    
    
            [[[ 6.9153e-02, -5.0437e-02, -1.0765e-02,  5.4134e-02,  2.2889e-02],
              [-7.2103e-03, -1.8056e-02, -6.1282e-02,  2.0499e-02,  6.9425e-02],
              [ 4.0183e-02, -1.5472e-02, -4.7119e-02, -8.3671e-03, -4.6837e-02],
              [ 1.2295e-02, -2.7917e-02,  3.3353e-02, -4.8480e-02,  7.8782e-02],
              [-3.4222e-03, -6.9079e-02, -6.3691e-02,  1.5120e-03,  4.1882e-02]],
    
             [[-6.8220e-02,  4.9157e-04,  1.3628e-02, -3.6824e-02,  3.9929e-02],
              [ 2.4355e-02, -7.5313e-02,  4.8031e-02, -4.7999e-02, -4.4971e-02],
              [-3.4287e-02, -1.6013e-02, -7.0799e-02, -4.8644e-02, -7.2285e-02],
              [-5.2659e-02, -3.2854e-02,  6.9199e-02,  8.0217e-02,  6.7537e-02],
              [-4.5917e-02,  7.4580e-02, -2.9394e-02, -8.2019e-03, -7.6555e-02]],
    
             [[ 1.3509e-02, -8.1356e-02, -8.1131e-02,  6.7206e-02,  4.6424e-02],
              [ 1.4622e-02,  6.4413e-02,  1.3079e-02, -5.0301e-02, -1.5072e-02],
              [-3.5176e-02, -6.4967e-02,  6.1386e-02,  6.2830e-03, -4.7690e-02],
              [ 1.8663e-02, -5.6275e-02, -6.8967e-02,  2.9964e-02, -1.9834e-02],
              [ 3.1835e-02, -3.3925e-02,  3.1836e-02,  4.2476e-02,  1.3856e-03]],
    
             [[ 3.0601e-02,  5.4959e-02, -6.5439e-02, -1.7944e-02,  6.6506e-02],
              [-5.3580e-02,  7.3758e-02, -2.9946e-02,  7.5885e-02,  7.6534e-02],
              [ 6.9067e-02,  4.4292e-02,  4.6791e-02,  7.0186e-02, -7.7546e-03],
              [-1.7692e-02,  5.1485e-02,  8.0205e-02,  6.2920e-02, -3.4527e-02],
              [ 8.0954e-02, -3.3190e-02,  2.8855e-02,  1.5492e-02, -4.8410e-02]],
    
             [[-3.5588e-02,  1.7428e-02, -4.2060e-03, -6.6630e-02,  5.6689e-02],
              [-6.7537e-02,  6.1356e-02, -6.1375e-02,  4.2452e-02, -1.4126e-02],
              [ 7.3965e-02, -6.0917e-02,  6.6971e-02, -3.0964e-02, -7.3984e-02],
              [-7.2435e-02,  5.4501e-02, -8.6379e-03,  2.0117e-02,  5.2748e-03],
              [-1.0984e-02, -7.6998e-02,  5.5423e-02,  2.9632e-02,  3.3179e-02]],
    
             [[-5.7820e-02, -7.1053e-02,  1.3702e-02,  4.0019e-03, -5.0106e-02],
              [-1.6269e-02, -7.9261e-02, -1.7431e-02, -1.9433e-02, -6.5844e-02],
              [-5.8382e-02,  5.1510e-02, -3.7572e-02,  1.0479e-02, -5.6084e-02],
              [ 7.2326e-02,  1.9664e-02,  5.9966e-02, -7.7922e-02,  7.7865e-02],
              [-8.6036e-03,  8.0995e-02, -6.8431e-02,  2.4838e-02,  6.3657e-02]]],
    
    
            [[[ 7.2776e-02, -7.0796e-02,  7.7569e-02, -1.6874e-02, -5.9874e-02],
              [-7.5478e-02, -3.3990e-02, -7.9262e-02, -4.7618e-02, -6.6682e-02],
              [-8.1314e-02,  4.9435e-02,  5.3615e-02,  1.4157e-02, -2.6105e-02],
              [-1.2609e-02,  6.0466e-02,  2.4350e-02,  7.3817e-02,  4.3128e-02],
              [ 9.4326e-03,  4.2357e-03, -4.0844e-02,  1.9739e-02, -3.2250e-02]],
    
             [[ 2.7404e-02, -7.6579e-02, -7.4173e-03,  5.8021e-02, -1.6821e-02],
              [ 2.6953e-02, -4.4245e-02,  1.4352e-02,  4.4509e-02, -2.0368e-02],
              [ 6.1575e-02, -7.0851e-02,  4.5446e-03,  4.0544e-03, -1.3090e-02],
              [-7.1353e-02, -7.8361e-02, -7.7491e-02,  3.1459e-02,  6.2964e-02],
              [ 7.2790e-02,  4.4566e-02,  4.2080e-02, -7.6178e-02,  3.3706e-02]],
    
             [[ 7.3229e-02, -7.6731e-03, -2.5042e-02,  4.3964e-02, -4.4970e-02],
              [-1.3054e-02,  5.5520e-02,  5.3743e-02,  5.0008e-02,  4.6876e-02],
              [-2.1199e-03, -4.1691e-02,  2.4833e-02, -2.1031e-02,  1.3680e-02],
              [ 2.1722e-02,  1.8856e-02,  6.6657e-02,  1.2442e-03, -6.2157e-02],
              [-5.4635e-02, -8.2455e-04,  1.3034e-03, -2.7799e-02,  5.9240e-02]],
    
             [[ 7.9368e-02,  6.9380e-02, -4.5395e-02, -6.1336e-02,  1.5119e-03],
              [-3.4830e-03,  8.1647e-02, -4.2134e-02,  6.8198e-02, -6.7456e-02],
              [-6.0014e-05, -7.1098e-02,  4.4846e-02,  1.8029e-02, -3.2867e-02],
              [-6.1457e-02,  7.0172e-02, -5.9548e-02,  6.8021e-02, -5.0176e-02],
              [-9.6529e-03,  7.1535e-03,  2.0087e-02,  2.8635e-02, -4.7184e-02]],
    
             [[-6.1210e-02, -7.4237e-02, -7.3227e-02, -1.9934e-02,  1.5572e-02],
              [-2.8551e-02, -4.1692e-02, -2.2897e-03,  4.6930e-02, -5.8657e-02],
              [ 3.3124e-02, -4.0890e-02, -5.7170e-03, -2.7143e-02, -8.0352e-02],
              [ 6.1452e-02,  6.6313e-02, -7.4846e-02,  3.7765e-02, -5.5715e-02],
              [-7.0193e-02,  5.6941e-02,  5.2043e-02,  1.9993e-02, -4.6263e-02]],
    
             [[-5.9085e-02,  3.1823e-02, -2.2031e-02, -1.7527e-02,  4.3893e-02],
              [ 5.7865e-03, -7.7265e-02, -4.1589e-02, -5.1208e-02,  3.3187e-02],
              [ 2.0969e-02, -7.7553e-02, -7.1897e-02, -8.1091e-02, -7.6785e-02],
              [-4.3439e-03,  1.2567e-02, -8.0016e-03,  7.3058e-02, -8.2255e-03],
              [ 1.2093e-02, -5.2143e-03,  1.4736e-02,  6.8717e-03,  5.8715e-02]]],
    
    
            ...,
    
    
            [[[-5.2194e-02, -4.2347e-03, -4.4263e-02,  2.4192e-02, -6.9166e-02],
              [ 6.0620e-02,  8.9869e-05,  3.7818e-02, -3.7148e-02,  3.7048e-02],
              [-7.3314e-02,  7.4280e-02,  3.2471e-02, -6.6439e-02, -8.1748e-03],
              [ 4.7723e-02, -1.8199e-02,  6.7297e-02,  2.3381e-02,  7.0248e-02],
              [-6.6225e-02,  1.2078e-02,  5.4304e-02, -6.0308e-03, -8.1403e-02]],
    
             [[ 6.5724e-02, -6.5975e-02,  2.1538e-02, -7.8015e-03, -1.5801e-04],
              [-2.3439e-02, -7.4369e-02,  1.3457e-02, -7.4700e-02, -4.8240e-02],
              [ 7.9284e-02,  3.3413e-02,  1.7630e-02, -5.1572e-02,  3.5144e-02],
              [ 3.3382e-02, -3.4528e-02,  2.8999e-02, -6.0313e-02, -7.0347e-02],
              [ 7.8021e-02,  3.8611e-02,  1.7424e-02,  2.7582e-02,  1.7013e-02]],
    
             [[ 3.3416e-02, -6.7426e-02, -5.8786e-02,  7.4535e-02, -4.1959e-02],
              [ 8.1446e-02, -6.5435e-02,  6.5071e-02, -1.3829e-02,  7.5820e-02],
              [ 4.1692e-02, -4.6965e-02, -4.8294e-02, -1.2342e-02, -7.0943e-02],
              [ 4.5588e-02, -2.9940e-02,  7.5104e-02,  8.7014e-04, -7.5588e-02],
              [-2.2450e-02, -4.8271e-02, -2.1748e-02,  2.5120e-02, -6.7030e-02]],
    
             [[-2.0097e-02,  1.1614e-02, -6.9385e-02,  7.1191e-02,  1.0714e-02],
              [ 5.3883e-02, -1.3728e-02,  6.7857e-03, -8.7429e-03, -5.0828e-02],
              [-4.3176e-03,  5.2082e-02, -7.4396e-02,  3.3515e-03,  7.6255e-02],
              [-7.3363e-02, -6.0923e-02,  2.9892e-02,  4.1304e-02,  9.1081e-03],
              [ 1.8744e-02,  4.6994e-02, -6.0415e-02,  2.9020e-02,  7.7318e-02]],
    
             [[ 6.6294e-02, -9.6177e-03,  3.7346e-02,  5.8365e-03, -4.4872e-02],
              [-6.0448e-03,  1.6768e-02, -4.5131e-02,  6.6709e-02,  7.5994e-02],
              [-1.2419e-02,  2.9443e-03,  4.8191e-02,  7.2325e-02,  1.0003e-02],
              [-5.9872e-02, -2.9491e-03,  2.4778e-02, -7.1743e-02, -7.1321e-02],
              [-8.9432e-03,  1.9837e-02,  7.7987e-02, -4.7855e-02, -3.7876e-02]],
    
             [[-5.9569e-02, -7.6702e-02,  6.4136e-03, -5.9631e-02,  6.4282e-02],
              [-6.3552e-02, -6.2936e-02, -5.7186e-02, -5.5413e-02,  7.8903e-02],
              [ 5.2268e-02, -9.5063e-03, -5.2273e-02, -1.6363e-02, -8.0039e-03],
              [-7.2972e-02, -5.7964e-02,  6.2399e-02,  3.8113e-02, -2.7751e-02],
              [-4.5329e-02,  7.5926e-02, -1.6094e-02, -3.6668e-02, -2.2976e-02]]],
    
    
            [[[-1.0882e-02,  5.6122e-02,  2.2949e-02,  2.6059e-02, -9.9854e-04],
              [ 5.7514e-02,  5.7161e-02, -1.4504e-02,  5.1846e-02, -2.3345e-02],
              [ 5.0875e-02,  4.9513e-02, -1.6447e-02, -5.2079e-02,  6.6205e-02],
              [-4.2524e-02,  3.4362e-02,  4.5988e-02,  3.4205e-02, -4.8662e-02],
              [-4.2669e-03, -3.8304e-02,  8.0862e-02, -2.9446e-02, -4.9828e-02]],
    
             [[ 7.8085e-02, -7.6506e-04, -5.4412e-02,  2.3751e-02, -5.9451e-02],
              [ 1.8386e-02, -4.8350e-03, -5.9541e-02,  7.8696e-02, -5.8644e-02],
              [ 4.9621e-03, -1.0025e-02,  3.5655e-02,  6.2835e-02, -5.6941e-02],
              [ 3.7193e-02, -4.6934e-02, -4.8452e-02, -1.8387e-02, -3.5100e-02],
              [ 2.8370e-02, -1.0693e-02, -1.6477e-02, -5.8145e-02, -3.6069e-02]],
    
             [[-2.2746e-02,  5.8963e-03,  1.8720e-02,  7.1307e-02,  1.1273e-02],
              [ 2.9105e-02,  4.3395e-02,  7.6044e-02, -7.4552e-02,  1.4571e-02],
              [-5.7467e-02, -1.8416e-02,  4.6946e-02,  3.7412e-02,  1.3316e-02],
              [-7.1574e-02, -3.7053e-02,  4.6213e-02, -5.1169e-02, -1.7142e-02],
              [ 7.4210e-02, -2.3602e-02, -3.0573e-02,  1.4261e-02,  3.2699e-02]],
    
             [[-2.7764e-02, -3.1217e-02,  2.3708e-02, -4.3135e-02, -3.0912e-02],
              [ 5.2089e-02, -4.4473e-02,  4.1630e-02,  8.0291e-03, -2.8991e-02],
              [-2.2166e-03, -7.7839e-02, -2.2548e-02, -1.7029e-02, -3.5430e-02],
              [ 6.3744e-02,  3.5856e-02, -2.2018e-03, -2.5444e-02, -1.7578e-02],
              [-2.3623e-02,  3.2102e-02, -7.3662e-02,  4.5218e-02,  4.5066e-02]],
    
             [[-7.4821e-02,  7.2776e-02, -3.4410e-02,  9.6637e-03,  6.4448e-02],
              [-4.9687e-02,  4.1663e-02, -6.4825e-02,  5.7106e-02,  4.4838e-02],
              [ 7.6672e-02, -4.2697e-02, -7.0403e-02, -5.9778e-02,  5.4729e-02],
              [ 2.5877e-02,  6.6603e-02,  7.0840e-02,  8.5048e-03,  1.3368e-02],
              [-2.8866e-02,  7.7623e-02,  2.3634e-02, -3.6336e-02,  3.5693e-02]],
    
             [[-2.7293e-02,  1.5551e-02, -3.8544e-02, -2.1937e-02, -3.4868e-02],
              [-8.0252e-02, -4.5072e-02,  7.6143e-02,  6.8013e-03, -5.5791e-02],
              [-5.5739e-02,  5.3459e-02,  6.4253e-03, -5.4221e-02,  4.0714e-02],
              [ 1.3440e-02, -3.7835e-02, -7.8627e-02,  4.2986e-02,  4.4575e-02],
              [ 3.6858e-02, -5.3311e-02,  4.2238e-02, -7.8721e-02,  6.4954e-02]]],
    
    
            [[[ 1.7418e-02, -8.0366e-02, -1.3253e-02,  4.4457e-02,  1.0547e-02],
              [ 3.7631e-02, -7.7365e-02,  4.4901e-02,  3.1770e-02,  3.1257e-02],
              [ 7.8002e-03,  4.8995e-02, -6.9652e-02,  2.8607e-02,  1.1077e-02],
              [ 8.7872e-03,  7.7718e-02, -6.1789e-03, -4.1345e-02,  5.6488e-02],
              [ 7.5392e-02, -2.1193e-02,  5.0872e-02, -2.0620e-03,  2.9076e-04]],
    
             [[ 1.3694e-02, -1.9251e-02, -4.2519e-02,  1.9778e-02,  5.6535e-02],
              [-7.4060e-02, -3.3801e-02,  6.9127e-02,  8.1294e-02,  2.0206e-02],
              [ 5.9165e-02,  5.8638e-02,  2.1704e-02,  6.3759e-03,  4.1365e-02],
              [-1.1626e-02,  8.1108e-03, -2.4373e-02,  3.0928e-02,  9.8471e-03],
              [ 4.9699e-02,  4.7595e-02, -7.9557e-02, -5.5499e-02,  6.0516e-02]],
    
             [[ 8.0727e-02,  4.6464e-02,  4.1355e-02, -5.3686e-02,  6.1809e-02],
              [-5.8222e-02, -7.6898e-02, -8.0850e-02, -8.1052e-03,  7.0742e-02],
              [-8.1679e-03, -5.1700e-04, -3.0686e-02,  5.1616e-02,  4.8269e-02],
              [-1.4156e-02, -7.8573e-02, -6.6673e-02, -6.3380e-03,  6.2212e-03],
              [-3.5517e-02, -7.3959e-02, -6.2262e-02,  6.2850e-03, -4.6077e-02]],
    
             [[ 1.8994e-02,  1.5611e-02, -1.9095e-02, -4.1216e-02, -5.4088e-02],
              [-6.1911e-02, -5.0086e-02,  7.2479e-02, -1.5800e-02,  4.5181e-02],
              [ 3.8523e-02, -4.4927e-02, -1.7534e-02, -7.5566e-02, -2.4905e-02],
              [ 2.5320e-02, -2.1148e-02,  5.6959e-02, -2.3006e-02,  2.6697e-02],
              [-5.8555e-02, -6.1362e-02,  4.0230e-02, -7.5364e-02, -7.3630e-03]],
    
             [[-5.7680e-02,  7.9182e-02, -4.0128e-02,  4.1789e-02, -5.7642e-02],
              [ 1.1695e-02,  4.9279e-02, -4.7486e-02, -6.2738e-03, -5.5410e-02],
              [ 5.5252e-02,  6.2000e-02,  2.2320e-02,  6.4926e-04,  5.6452e-02],
              [-2.6899e-02,  5.9691e-02, -3.0709e-02, -3.8463e-03,  4.7551e-02],
              [ 4.3603e-02,  7.5724e-02,  4.9744e-02,  5.9796e-02, -5.4049e-02]],
    
             [[ 9.8489e-03, -4.0123e-02,  7.4058e-02, -3.2682e-02,  3.5798e-02],
              [ 5.3772e-03, -6.0099e-02, -4.8092e-02,  2.9152e-02, -7.8119e-02],
              [-6.6541e-02,  7.6296e-02, -5.8651e-02, -4.5865e-02, -4.5619e-02],
              [ 3.5178e-02,  7.0461e-02,  6.3369e-02,  4.2826e-02, -4.9346e-02],
              [ 7.5675e-02, -4.9870e-02,  7.1649e-02, -7.9777e-02, -6.7924e-02]]]],
           requires_grad=True)




```python
net.fc1.weight
```




    Parameter containing:
    tensor([[-0.0180, -0.0246,  0.0405,  ..., -0.0101, -0.0646, -0.0463],
            [-0.0059,  0.0684, -0.0468,  ..., -0.0628,  0.0501, -0.0287],
            [ 0.0374, -0.0466, -0.0234,  ..., -0.0011, -0.0185,  0.0707],
            ...,
            [-0.0415, -0.0526,  0.0653,  ..., -0.0244, -0.0301,  0.0674],
            [-0.0145,  0.0131,  0.0358,  ..., -0.0164,  0.0235,  0.0477],
            [-0.0556,  0.0365, -0.0500,  ...,  0.0173, -0.0557, -0.0519]],
           requires_grad=True)




```python
net.fc2.weight
```




    Parameter containing:
    tensor([[ 0.0627,  0.0617, -0.0270,  ..., -0.0502,  0.0709,  0.0121],
            [ 0.0290, -0.0349, -0.0037,  ...,  0.0099,  0.0760,  0.0471],
            [-0.0398, -0.0793,  0.0338,  ...,  0.0718, -0.0277, -0.0903],
            ...,
            [ 0.0624,  0.0083,  0.0817,  ...,  0.0203,  0.0857,  0.0103],
            [ 0.0567,  0.0529, -0.0337,  ..., -0.0579, -0.0091,  0.0211],
            [ 0.0007, -0.0308,  0.0113,  ..., -0.0188, -0.0500, -0.0062]],
           requires_grad=True)




```python
net.out.weight
```




    Parameter containing:
    tensor([[ 0.0733,  0.0524,  0.0828,  0.1119,  0.1230, -0.0773,  0.0818, -0.0935,
              0.0033,  0.0650, -0.1098,  0.0977,  0.1096,  0.0394,  0.0429,  0.0261,
             -0.0279, -0.0036,  0.0285, -0.0218,  0.0024, -0.0762, -0.0176, -0.0046,
             -0.1010,  0.0509,  0.1077, -0.0064, -0.1042, -0.0110, -0.0101,  0.0548,
              0.0124, -0.0464, -0.0940,  0.0932,  0.0786, -0.0351,  0.0648, -0.0113,
              0.0633,  0.0332,  0.1036,  0.1038, -0.0624,  0.0958,  0.0794,  0.0009,
              0.1066, -0.1241, -0.0833,  0.0288,  0.0699, -0.0270, -0.0575, -0.0576,
             -0.0575,  0.0522, -0.0663,  0.0565],
            [-0.0342, -0.1035,  0.1005, -0.0552, -0.0598,  0.0884,  0.1068,  0.0868,
              0.1048, -0.0731, -0.1002,  0.0724, -0.1216,  0.1040, -0.1154, -0.0746,
              0.0430,  0.0030,  0.1192,  0.0795, -0.0986,  0.1225,  0.0535, -0.1106,
              0.0579,  0.0028,  0.1008,  0.0885, -0.0996,  0.1286,  0.0334, -0.0493,
             -0.0530,  0.1274,  0.0335, -0.0589, -0.0105,  0.1196, -0.1052,  0.0128,
             -0.1138,  0.0043, -0.0722, -0.0555, -0.0836, -0.0775,  0.0881, -0.1057,
              0.1117,  0.1264,  0.0064,  0.1198, -0.0016, -0.0968, -0.1149,  0.1085,
             -0.0333, -0.0660, -0.1161, -0.1142],
            [-0.0446, -0.0757,  0.0260,  0.0121, -0.0406, -0.0385,  0.1200, -0.0705,
             -0.0287, -0.0998, -0.0697,  0.1255,  0.0522,  0.0538,  0.0642,  0.0323,
              0.0738, -0.0220,  0.0392, -0.1158,  0.0493, -0.1278, -0.0180, -0.0392,
              0.0676,  0.0234,  0.0614, -0.0942,  0.0107, -0.0551, -0.0292,  0.1131,
             -0.0417, -0.1034,  0.1020,  0.0914,  0.0905, -0.0267,  0.0938,  0.1259,
              0.0680, -0.0529, -0.0466,  0.0904, -0.1183, -0.0183, -0.0019,  0.1266,
             -0.0134,  0.0560, -0.0176, -0.1001,  0.0491, -0.0369, -0.1029, -0.0375,
              0.0116, -0.0415,  0.0282,  0.1229],
            [ 0.1004,  0.0098,  0.0348, -0.0891, -0.0047,  0.0936,  0.0980, -0.1096,
             -0.1263,  0.0034,  0.0696, -0.0577,  0.0275,  0.0707,  0.0642, -0.0145,
              0.0383,  0.1209,  0.0425,  0.0738,  0.0600,  0.0388,  0.0613, -0.0692,
             -0.0990,  0.1277,  0.0797,  0.0322, -0.0081,  0.0799, -0.0036, -0.0209,
             -0.0566, -0.1072,  0.0981,  0.1217,  0.0756, -0.1109,  0.0471,  0.0233,
              0.0730,  0.0127, -0.0935,  0.0574,  0.1152, -0.0627, -0.0329, -0.0505,
              0.1088, -0.1026,  0.1171,  0.1264, -0.0680,  0.0444, -0.0999, -0.0257,
             -0.1154,  0.0860,  0.0749, -0.0368],
            [-0.0977,  0.1266,  0.1208, -0.0107, -0.0439, -0.0593, -0.0853,  0.0080,
             -0.0386,  0.1145, -0.1181,  0.0789,  0.0332, -0.0295,  0.0490, -0.0170,
             -0.0299, -0.0096,  0.0871, -0.0800,  0.0190,  0.0751, -0.0220,  0.0657,
             -0.0595, -0.0858,  0.0425, -0.0583,  0.0294, -0.1013,  0.0199,  0.1287,
              0.0070, -0.1214, -0.0017,  0.0742,  0.0871, -0.0727,  0.1194,  0.0474,
             -0.0214,  0.0500,  0.0038, -0.0439, -0.0009, -0.0892, -0.0990,  0.1285,
             -0.0796,  0.0500, -0.0847, -0.0756, -0.1246, -0.0341, -0.0828, -0.0079,
              0.0919, -0.0451,  0.0340,  0.0433],
            [-0.0606, -0.0467,  0.0986,  0.0584,  0.0979, -0.0156,  0.0088,  0.0206,
              0.0289,  0.0748,  0.0323,  0.1201, -0.0821,  0.0315, -0.1023, -0.0977,
             -0.0900,  0.1165, -0.0736, -0.0668, -0.0906, -0.0565, -0.0898, -0.0970,
             -0.0463,  0.1164,  0.0546, -0.0476,  0.0326,  0.0214, -0.0212,  0.1134,
              0.1051, -0.0408, -0.1153,  0.0313, -0.0370, -0.1213, -0.1226,  0.0602,
             -0.0164,  0.0377,  0.0880, -0.0161,  0.0457, -0.0338, -0.1047, -0.0682,
             -0.0525,  0.0240, -0.1143,  0.0474, -0.0900, -0.0638,  0.1102, -0.1214,
              0.0807,  0.0132, -0.0951, -0.1156],
            [-0.0366,  0.0161, -0.1193, -0.0200, -0.0967,  0.1263,  0.0110, -0.0186,
             -0.0153,  0.0563, -0.0759, -0.0153,  0.1065, -0.0463, -0.1114,  0.0412,
              0.1071, -0.0165,  0.1187, -0.1035, -0.1170,  0.0296, -0.0993, -0.1045,
              0.0828, -0.0391, -0.0807, -0.0597,  0.0548,  0.0789,  0.0515, -0.0155,
              0.0530, -0.0511,  0.0697,  0.1208,  0.1264,  0.0853, -0.0356, -0.1068,
              0.0402,  0.1238,  0.0278,  0.0020,  0.0350, -0.0407, -0.1257, -0.0295,
             -0.0517,  0.0404,  0.0105,  0.0483,  0.0232,  0.0534, -0.0443, -0.0328,
             -0.1095, -0.0732, -0.0665,  0.0176],
            [-0.1120,  0.0148,  0.0271,  0.0124, -0.0642,  0.1260, -0.0390, -0.1215,
              0.0188,  0.0426, -0.0887, -0.0744, -0.0501, -0.0738, -0.0518,  0.0187,
              0.0429, -0.1134, -0.1246,  0.0136, -0.0727,  0.0133, -0.0330, -0.1037,
             -0.1091,  0.0318, -0.0723,  0.0788,  0.0580,  0.0151, -0.0268, -0.0316,
             -0.0713, -0.0636,  0.1021, -0.0730, -0.0653, -0.0539, -0.0408, -0.0536,
              0.0459,  0.0306,  0.0868,  0.1035,  0.0904,  0.0423,  0.0794,  0.1114,
              0.1067, -0.0576, -0.0715,  0.1020, -0.0272,  0.1087,  0.0382, -0.0756,
             -0.1195, -0.0467, -0.0447,  0.1086],
            [-0.0735, -0.1172,  0.0289, -0.1214,  0.0745, -0.0134, -0.0665, -0.1081,
             -0.0690, -0.0520,  0.0041, -0.0306,  0.1179,  0.0657, -0.0665,  0.0974,
             -0.0739,  0.1126, -0.0260, -0.1224, -0.1032,  0.0600,  0.1276, -0.0682,
             -0.1194, -0.0620,  0.0501, -0.0787,  0.0407, -0.0352, -0.0004, -0.0818,
              0.0168, -0.1078,  0.1057, -0.0510,  0.0148, -0.0590, -0.0260,  0.1129,
              0.0934,  0.0169, -0.0635, -0.0390,  0.0400,  0.0386,  0.1100,  0.0031,
             -0.0387,  0.0613, -0.0168,  0.0699,  0.0017,  0.0066,  0.0976,  0.0912,
             -0.0551,  0.0896,  0.0323,  0.0224],
            [-0.0280, -0.0688,  0.0529,  0.0128,  0.0540,  0.0881,  0.0082,  0.0788,
              0.0383,  0.0258, -0.0264, -0.0664, -0.0458,  0.0258, -0.0416, -0.0282,
             -0.0783,  0.0659,  0.1049, -0.0210,  0.1108,  0.0627, -0.0276,  0.0427,
             -0.0491, -0.0912,  0.0304, -0.0528,  0.0520,  0.0063, -0.0330,  0.0074,
             -0.0493, -0.0834,  0.0106,  0.0899, -0.0032,  0.0217, -0.0310, -0.0475,
              0.0554,  0.0439, -0.0888,  0.0877, -0.0226, -0.0083,  0.0622,  0.0391,
              0.0277,  0.0902,  0.0793, -0.0636, -0.0550, -0.0400, -0.0605, -0.0293,
             -0.0190, -0.0431,  0.0557, -0.0458]], requires_grad=True)



Each layers is going to have a different tensor shape, lets look at those:


```python
net.conv1.weight.shape #6 5x5 kernels with 1 depth
```




    torch.Size([6, 1, 5, 5])




```python
net.conv2.weight.shape #12 5x5 kernels with 6 depth
```




    torch.Size([12, 6, 5, 5])




```python
net.fc1.weight.shape #rank 2 tensor with first axis corresponding to the out_feature length and 
# second axis corresponding to in_feature size
```




    torch.Size([120, 192])




```python
net.fc2.weight.shape

```




    torch.Size([60, 120])




```python
net.out.weight.shape
```




    torch.Size([10, 60])




```python
net.conv1.weight[0].shape #single filter
```




    torch.Size([1, 5, 5])




```python
net.conv2.weight[0].shape #single filter of second convolutional layer
```




    torch.Size([6, 5, 5])



#### Accsessing parameters of the network


```python
for param in net.parameters():
    print(param.shape)
```

    torch.Size([6, 1, 5, 5])
    torch.Size([6])
    torch.Size([12, 6, 5, 5])
    torch.Size([12])
    torch.Size([120, 192])
    torch.Size([120])
    torch.Size([60, 120])
    torch.Size([60])
    torch.Size([10, 60])
    torch.Size([10])
    


```python
for name, param in net.named_parameters():
    print(name, "\t\t", param.shape)
```

    conv1.weight 		 torch.Size([6, 1, 5, 5])
    conv1.bias 		 torch.Size([6])
    conv2.weight 		 torch.Size([12, 6, 5, 5])
    conv2.bias 		 torch.Size([12])
    fc1.weight 		 torch.Size([120, 192])
    fc1.bias 		 torch.Size([120])
    fc2.weight 		 torch.Size([60, 120])
    fc2.bias 		 torch.Size([60])
    out.weight 		 torch.Size([10, 60])
    out.bias 		 torch.Size([10])
    

#### Matrix multiplication in pytorch


```python
weight_mat = torch.tensor([1,2,3,4], dtype = torch.float32)
```


```python
input_feature = torch.tensor([
    [2,3,4,5],
    [6,7,8,9],
    [10,11,12,13],
], dtype = torch.float32)
```


```python
input_feature.matmul(weight_mat) #4d to 3d
```




    tensor([ 40.,  80., 120.])




```python
fc = nn.Linear(in_features=4, out_features =3) #creates an internal 3x4 weight matrix (parameter class instance)
```


```python
fc.weight = nn.Parameter(weight_mat)
```


```python
fc.weight
```




    Parameter containing:
    tensor([1., 2., 3., 4.], requires_grad=True)




```python
fc
```




    Linear(in_features=4, out_features=3, bias=True)




```python
t = torch.tensor([1,3,5,4], dtype = torch.float32)
```

#### Linear Layers


```python
in_features = torch.tensor([1,2,3,4], dtype = torch.float32)
```


```python
weight_matrix = torch.tensor([
    [1,2,3,4],
    [2,3,4,5],
    [3,4,5,6]
], dtype = torch.float32)
```


```python
fx = nn.Linear(in_features = 4, out_features =3)
```


```python
weight_matrix.matmul(in_features)
```




    tensor([30., 40., 50.])




```python
fx(in_features)
```




    tensor([-2.2189,  1.9540,  0.9856], grad_fn=<AddBackward0>)




```python
fx.weight = nn.Parameter(weight_matrix)
```


```python
fx(in_features) #bias on
```




    tensor([29.7179, 40.2425, 49.9494], grad_fn=<AddBackward0>)




```python
fx = nn.Linear(in_features = 4, out_features =3, bias = False)
```


```python
fx.weight = nn.Parameter(weight_matrix)
```


```python
fx(in_features) #bias off, same result as weight_matrix.matmul(in_feature)
```




    tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)




```python

```
